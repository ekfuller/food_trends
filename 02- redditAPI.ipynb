{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1e18e0-9066-407b-a69c-26da694871d8",
   "metadata": {},
   "source": [
    "### Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dabb7d-047e-4547-859a-edd9affdf8ad",
   "metadata": {},
   "source": [
    "The information pulled from the pushshift API included inaccurate values for post scores and post number of comments. This notebooks uses the Reddit API to pull in the correct scores and number of comments for the post ids sourced earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c9fd7b-7ffc-46f5-975c-70e17b3f1e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Downloading praw-7.6.0-py3-none-any.whl (188 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188 kB 6.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting update-checker>=0.18\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Collecting websocket-client>=0.54.0\n",
      "  Using cached websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
      "Collecting prawcore<3,>=2.1\n",
      "  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /Users/emilyfuller/opt/anaconda3/lib/python3.9/site-packages (from prawcore<3,>=2.1->praw) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/emilyfuller/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/emilyfuller/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/emilyfuller/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/emilyfuller/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.2)\n",
      "Installing collected packages: websocket-client, update-checker, prawcore, praw\n",
      "Successfully installed praw-7.6.0 prawcore-2.3.0 update-checker-0.18.0 websocket-client-1.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f46753ca-ec57-4e8b-9634-bb80ddd56f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime as dt\n",
    "import praw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede2556d-67db-4cd3-a846-652320ccc71a",
   "metadata": {},
   "source": [
    "To get these credentials I applied for an api key and ran curl commands in my terminal(CLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90974033-2e94-45df-8b4c-d92ebf0d13fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"yEQokwxZEpfab7-hjZGYqQ\",\n",
    "    client_secret=\"0wfgItj8y8K12zwbKGBWEQantbeK8A\",\n",
    "    password=\"capstone22\",\n",
    "    user_agent=\"food trends 2 by u/ekfuller\",\n",
    "    username=\"ekfuller\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca0d2ae-cfed-4dbe-aca0-95ccc6d14b77",
   "metadata": {},
   "source": [
    "If the following cell runs then the credentials are active and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba942a7e-a74c-4b2e-b35d-fb6b2d4998f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v84rwq 9507 798\n",
      "v8hj5f 348 8\n",
      "v8ajkp 932 27\n",
      "v7xvwo 6556 153\n",
      "v8h72o 169 15\n",
      "v8gcvu 117 6\n",
      "v82k01 1021 32\n",
      "v7n7ws 15203 335\n",
      "v83cn5 790 32\n",
      "v8djfp 108 7\n",
      "v8f6o9 69 7\n",
      "v8bojd 143 22\n",
      "v8hoae 38 0\n",
      "v83qv7 479 15\n",
      "v8hd6k 33 3\n",
      "v8jfpw 24 1\n",
      "v8dx18 56 0\n",
      "v81h1u 523 19\n",
      "v8ij83 27 12\n",
      "v7v21n 1111 37\n",
      "v8c1iv 59 7\n",
      "v8irib 17 3\n",
      "v8dg42 42 2\n",
      "v86li2 153 3\n",
      "v8jhab 15 10\n"
     ]
    }
   ],
   "source": [
    "for submission in reddit.subreddit(\"food\").hot(limit=25):\n",
    "    print(submission.id, submission.score, submission.num_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a065d42-3d7d-409f-a8ca-87975eff87d5",
   "metadata": {},
   "source": [
    "# read in reddit data from csv to get post ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccb97cdf-d937-4d32-a28b-9a8b1cd1a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = pd.read_csv('../food_trends/Data/reddit_6_months.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0e324f2-3225-4060-8dff-607c554c5feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uu6g0w</td>\n",
       "      <td>food</td>\n",
       "      <td>[homemade] Polynesian (Chick-Fil-A sauce) chic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1653077037</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uu6cni</td>\n",
       "      <td>food</td>\n",
       "      <td>[I ate] Scotch mutton pie, pub in Edinburgh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1653076799</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id subreddit                                              title  \\\n",
       "0  uu6g0w      food  [homemade] Polynesian (Chick-Fil-A sauce) chic...   \n",
       "1  uu6cni      food        [I ate] Scotch mutton pie, pub in Edinburgh   \n",
       "\n",
       "  selftext  created_utc  num_comments  score  \n",
       "0      NaN   1653077037             0      1  \n",
       "1      NaN   1653076799             0      1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6f18b7b-dc8f-4d85-a4f0-5bea4d21e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df.drop(columns=['subreddit','selftext', 'num_comments','score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddf60b1d-d804-410a-b6bb-cfba8e1e847a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uu6g0w</td>\n",
       "      <td>[homemade] Polynesian (Chick-Fil-A sauce) chic...</td>\n",
       "      <td>1653077037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uu6cni</td>\n",
       "      <td>[I ate] Scotch mutton pie, pub in Edinburgh</td>\n",
       "      <td>1653076799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uu6apo</td>\n",
       "      <td>[homemade] ðŸ‡²ðŸ‡¦</td>\n",
       "      <td>1653076639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uu644e</td>\n",
       "      <td>[homemade] Chilli Paneer, Spinach, Potatoes wi...</td>\n",
       "      <td>1653076091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uu5x2y</td>\n",
       "      <td>[Homemade] Tart - Salmon, spinach and goat cheese</td>\n",
       "      <td>1653075500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50172</th>\n",
       "      <td>r8dw7i</td>\n",
       "      <td>[homemade] Korean beef lettuce wraps</td>\n",
       "      <td>1638580177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50173</th>\n",
       "      <td>r8du3q</td>\n",
       "      <td>[homemade] Ramen. Inexperienced cook and Iâ€™m p...</td>\n",
       "      <td>1638580002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50174</th>\n",
       "      <td>r8dsud</td>\n",
       "      <td>[homemade] I am a very inexperienced cook and ...</td>\n",
       "      <td>1638579895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50175</th>\n",
       "      <td>r8dquz</td>\n",
       "      <td>[Homemade] Beef stew w/ fresh baked bread</td>\n",
       "      <td>1638579723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50176</th>\n",
       "      <td>r8dmi8</td>\n",
       "      <td>[homemade] thanksgiving leftover turkey and bi...</td>\n",
       "      <td>1638579365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50177 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  created_utc\n",
       "0      uu6g0w  [homemade] Polynesian (Chick-Fil-A sauce) chic...   1653077037\n",
       "1      uu6cni        [I ate] Scotch mutton pie, pub in Edinburgh   1653076799\n",
       "2      uu6apo                                      [homemade] ðŸ‡²ðŸ‡¦   1653076639\n",
       "3      uu644e  [homemade] Chilli Paneer, Spinach, Potatoes wi...   1653076091\n",
       "4      uu5x2y  [Homemade] Tart - Salmon, spinach and goat cheese   1653075500\n",
       "...       ...                                                ...          ...\n",
       "50172  r8dw7i               [homemade] Korean beef lettuce wraps   1638580177\n",
       "50173  r8du3q  [homemade] Ramen. Inexperienced cook and Iâ€™m p...   1638580002\n",
       "50174  r8dsud  [homemade] I am a very inexperienced cook and ...   1638579895\n",
       "50175  r8dquz          [Homemade] Beef stew w/ fresh baked bread   1638579723\n",
       "50176  r8dmi8  [homemade] thanksgiving leftover turkey and bi...   1638579365\n",
       "\n",
       "[50177 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e1f1c5-5897-4ea1-ba03-a3781c18c477",
   "metadata": {},
   "source": [
    "Using the post ids, pull the score and number of comments per post. It gets broken into 100 post chunks to comply with pull limits. I further broke the data into groups of 1000 since the function takes a long time to run and it was easier to save in chunks and then recombine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd0ef707-93c6-4e3d-9017-675e35a3d340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(posts):\n",
    "    for submission in range(0,len(posts)):\n",
    "            ids.append(posts[submission])\n",
    "            scores.append(reddit.submission(posts[submission]).score)\n",
    "            comments.append(reddit.submission(posts[submission]).num_comments)\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d5d0f17-05d7-4e40-a17a-22c8b4fcae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lists(chunks):\n",
    "    list_of_chunks = []\n",
    "    for x in range(0,len(chunks)):\n",
    "        list_of_chunks.append(list(chunks[x]))\n",
    "    return list_of_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01ce28d3-afc8-48b9-adae-0406d4803ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "scores = []\n",
    "comments = []\n",
    "def get_posts(items):\n",
    "    for i in range(0, len(items)):\n",
    "        posts = items[i]\n",
    "        for submission in range(0,len(posts)):\n",
    "            ids.append(posts[submission])\n",
    "            scores.append(reddit.submission(posts[submission]).score)\n",
    "            comments.append(reddit.submission(posts[submission]).num_comments)\n",
    "    return ids, scores, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8f0dec20-52f4-4e79-8612-4b674c85f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df_10 = posts_df[:1000]\n",
    "\n",
    "chunks = [posts_df_10['id'][x:x+100] for x in range(0, len(posts_df_10), 100)]\n",
    "\n",
    "increments_10 = get_lists(chunks)\n",
    "\n",
    "get_posts(increments_10)\n",
    "\n",
    "scores_df_10 = pd.DataFrame([ids, scores, comments]).T\n",
    "\n",
    "scores_df_10.rename(columns={0:'id', 1:'score', 2:'comments'}, inplace=True)\n",
    "scores_df_10.to_csv('./Data/0_to_10k_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4793503-62f6-4a57-86ca-bcbfeb704f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df_20 = posts_df[1000:2000]\n",
    "\n",
    "chunks = [posts_df_20['id'][x:x+100] for x in range(0, len(posts_df_20), 100)]\n",
    "\n",
    "increments_20 = get_lists(chunks)\n",
    "\n",
    "get_posts(increments_20)\n",
    "\n",
    "scores_df_20 = pd.DataFrame([ids, scores, comments]).T\n",
    "\n",
    "scores_df_20.rename(columns={0:'id', 1:'score', 2:'comments'}, inplace=True)\n",
    "scores_df_20\n",
    "\n",
    "scores_df_20.to_csv('./Data/10_to_20k_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5c1b91a-4e10-4299-83aa-640f9176101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "scores = []\n",
    "comments = []\n",
    "def get_posts(items):\n",
    "    for i in range(0, len(items)):\n",
    "        posts = items[i]\n",
    "        for submission in range(0,len(posts)):\n",
    "            ids.append(posts[submission])\n",
    "            scores.append(reddit.submission(posts[submission]).score)\n",
    "            comments.append(reddit.submission(posts[submission]).num_comments)\n",
    "    return ids, scores, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16a812fc-7714-43dd-b0df-b934ee988d97",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "posts_df_30 = posts_df[2000:3000]\n",
    "\n",
    "chunks = [posts_df_30['id'][x:x+100] for x in range(0, len(posts_df_30), 100)]\n",
    "\n",
    "increments_30 = get_lists(chunks)\n",
    "\n",
    "get_posts(increments_30)\n",
    "\n",
    "scores_df_30 = pd.DataFrame([ids, scores, comments]).T\n",
    "\n",
    "scores_df_30.rename(columns={0:'id', 1:'score', 2:'comments'}, inplace=True)\n",
    "scores_df_30.to_csv('./Data/2_to_3k_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6aa2a0d5-1922-489e-9ba1-8a7d05009a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "scores = []\n",
    "comments = []\n",
    "def get_posts(items):\n",
    "    for i in range(0, len(items)):\n",
    "        posts = items[i]\n",
    "        for submission in range(0,len(posts)):\n",
    "            ids.append(posts[submission])\n",
    "            scores.append(reddit.submission(posts[submission]).score)\n",
    "            comments.append(reddit.submission(posts[submission]).num_comments)\n",
    "    return ids, scores, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19c14c8f-5cda-4563-ac5e-c7921131c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df_40 = posts_df[3000:4000]\n",
    "\n",
    "chunks = [posts_df_40['id'][x:x+100] for x in range(0, len(posts_df_40), 100)]\n",
    "\n",
    "increments_40 = get_lists(chunks)\n",
    "\n",
    "get_posts(increments_40)\n",
    "\n",
    "scores_df_40 = pd.DataFrame([ids, scores, comments]).T\n",
    "\n",
    "scores_df_40.rename(columns={0:'id', 1:'score', 2:'comments'}, inplace=True)\n",
    "\n",
    "scores_df_40.to_csv('./Data/3_to_4k_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06f05751-78ff-4535-b23f-6e218f46b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "scores = []\n",
    "comments = []\n",
    "def get_posts(items):\n",
    "    for i in range(0, len(items)):\n",
    "        posts = items[i]\n",
    "        for submission in range(0,len(posts)):\n",
    "            ids.append(posts[submission])\n",
    "            scores.append(reddit.submission(posts[submission]).score)\n",
    "            comments.append(reddit.submission(posts[submission]).num_comments)\n",
    "    return ids, scores, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cdd7bf92-1817-4770-8708-14ce493649a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df_50 = posts_df[4000:5000]\n",
    "\n",
    "chunks = [posts_df_50['id'][x:x+100] for x in range(0, len(posts_df_50), 100)]\n",
    "\n",
    "increments_50 = get_lists(chunks)\n",
    "\n",
    "get_posts(increments_50)\n",
    "\n",
    "scores_df_50 = pd.DataFrame([ids, scores, comments]).T\n",
    "\n",
    "scores_df_50.rename(columns={0:'id', 1:'score', 2:'comments'}, inplace=True)\n",
    "scores_df_50.to_csv('./Data/4_to_5k_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86e84b25-38f8-4738-8cd0-13b526e7c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "scores = []\n",
    "comments = []\n",
    "def get_posts(items):\n",
    "    for i in range(0, len(items)):\n",
    "        posts = items[i]\n",
    "        for submission in range(0,len(posts)):\n",
    "            ids.append(posts[submission])\n",
    "            scores.append(reddit.submission(posts[submission]).score)\n",
    "            comments.append(reddit.submission(posts[submission]).num_comments)\n",
    "    return ids, scores, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c95dfbf-1838-4d57-b86c-9e927c287e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df_60 = posts_df[5000:6000]\n",
    "\n",
    "chunks = [posts_df_60['id'][x:x+100] for x in range(0, len(posts_df_60), 100)]\n",
    "\n",
    "increments_60 = get_lists(chunks)\n",
    "\n",
    "get_posts(increments_60)\n",
    "\n",
    "scores_df_60 = pd.DataFrame([ids, scores, comments]).T\n",
    "\n",
    "scores_df_60.rename(columns={0:'id', 1:'score', 2:'comments'}, inplace=True)\n",
    "scores_df_60.to_csv('./Data/5_to_6k_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d4befe0-d109-4be7-ad8e-fee00fd5ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "scores = []\n",
    "comments = []\n",
    "def get_posts(items):\n",
    "    for i in range(0, len(items)):\n",
    "        posts = items[i]\n",
    "        for submission in range(0,len(posts)):\n",
    "            ids.append(posts[submission])\n",
    "            scores.append(reddit.submission(posts[submission]).score)\n",
    "            comments.append(reddit.submission(posts[submission]).num_comments)\n",
    "    return ids, scores, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ddf90eb5-9318-44cd-bd71-0ae4f27a70fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df_70 = posts_df[6000:7000]\n",
    "\n",
    "chunks = [posts_df_70['id'][x:x+100] for x in range(0, len(posts_df_70), 100)]\n",
    "\n",
    "increments_70 = get_lists(chunks)\n",
    "\n",
    "get_posts(increments_70)\n",
    "\n",
    "scores_df_70 = pd.DataFrame([ids, scores, comments]).T\n",
    "\n",
    "scores_df_70.rename(columns={0:'id', 1:'score', 2:'comments'}, inplace=True)\n",
    "scores_df_70.to_csv('./Data/7_to_7k_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ac20a87-09b8-4a34-9c27-7667f7468b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "scores = []\n",
    "comments = []\n",
    "def get_posts(items):\n",
    "    for i in range(0, len(items)):\n",
    "        posts = items[i]\n",
    "        for submission in range(0,len(posts)):\n",
    "            ids.append(posts[submission])\n",
    "            scores.append(reddit.submission(posts[submission]).score)\n",
    "            comments.append(reddit.submission(posts[submission]).num_comments)\n",
    "    return ids, scores, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55fb7c7a-2026-44ae-a12d-6eda9e298ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df_80 = posts_df[7000:8000]\n",
    "\n",
    "chunks = [posts_df_80['id'][x:x+100] for x in range(0, len(posts_df_80), 100)]\n",
    "\n",
    "increments_80 = get_lists(chunks)\n",
    "\n",
    "get_posts(increments_80)\n",
    "\n",
    "scores_df_80 = pd.DataFrame([ids, scores, comments]).T\n",
    "\n",
    "scores_df_80.rename(columns={0:'id', 1:'score', 2:'comments'}, inplace=True)\n",
    "scores_df_80.to_csv('./Data/7_to_8k_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28ce3d2c-c9cf-402b-82da-7c73c435c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "scores = []\n",
    "comments = []\n",
    "def get_posts(items):\n",
    "    for i in range(0, len(items)):\n",
    "        posts = items[i]\n",
    "        for submission in range(0,len(posts)):\n",
    "            ids.append(posts[submission])\n",
    "            scores.append(reddit.submission(posts[submission]).score)\n",
    "            comments.append(reddit.submission(posts[submission]).num_comments)\n",
    "    return ids, scores, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b1cb8c4-56ce-422a-9622-fe8553927c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df_90 = posts_df[8000:9000]\n",
    "\n",
    "chunks = [posts_df_90['id'][x:x+100] for x in range(0, len(posts_df_90), 100)]\n",
    "\n",
    "increments_90 = get_lists(chunks)\n",
    "\n",
    "get_posts(increments_90)\n",
    "\n",
    "scores_df_90 = pd.DataFrame([ids, scores, comments]).T\n",
    "\n",
    "scores_df_90.rename(columns={0:'id', 1:'score', 2:'comments'}, inplace=True)\n",
    "scores_df_90.to_csv('./Data/8_to_9k_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8d97257-4231-4f67-bb9f-ed2d0b46a271",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "scores = []\n",
    "comments = []\n",
    "def get_posts(items):\n",
    "    for i in range(0, len(items)):\n",
    "        posts = items[i]\n",
    "        for submission in range(0,len(posts)):\n",
    "            ids.append(posts[submission])\n",
    "            scores.append(reddit.submission(posts[submission]).score)\n",
    "            comments.append(reddit.submission(posts[submission]).num_comments)\n",
    "    return ids, scores, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15aa1b77-2c20-4211-b94c-fcdc018ff2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df_100 = posts_df[9000:10001]\n",
    "\n",
    "chunks = [posts_df_100['id'][x:x+100] for x in range(0, len(posts_df_100), 100)]\n",
    "\n",
    "increments_100 = get_lists(chunks)\n",
    "\n",
    "get_posts(increments_100)\n",
    "\n",
    "scores_df_100 = pd.DataFrame([ids, scores, comments]).T\n",
    "\n",
    "scores_df_100.rename(columns={0:'id', 1:'score', 2:'comments'}, inplace=True)\n",
    "scores_df_100.to_csv('./Data/9_to_10k_scores.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
